{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source\n",
    "\n",
    "kagle  (https://www.kaggle.com/datasets/blastchar/telco-customer-churn/download?datasetVersionNumber=1)\n",
    "\n",
    "github (https://github.com/Briankimany/data-science/blob/main/week%201/churn.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-10-01 15:47:34.230258: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-01 15:47:39.253982: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-01 15:47:39.266995: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-01 15:47:49.756171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RELEVANT COLUMNS\n",
    "> CreditScore—can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n",
    "\n",
    "> Geography—a customer’s location can affect their decision to leave the bank.\n",
    "\n",
    "> Gender—it’s interesting to explore whether gender plays a role in a customer leaving the bank.\n",
    "\n",
    "> Age—this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n",
    "\n",
    "> Tenure—refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n",
    "\n",
    "> Balance—also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n",
    "\n",
    "> HasCrCard—denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n",
    "\n",
    "> IsActiveMember—active customers are less likely to leave the bank.\n",
    "\n",
    "> EstimatedSalary—as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n",
    "\n",
    "> Exited (1(y), o(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/brian/PycharmProjects/allison learing/churn.csv\")\n",
    "data_1 = pd.read_csv(\"/home/brian/PycharmProjects/allison learing/churn.csv\")\n",
    "unused_list = ['RowNumber',\t'CustomerId',\t'Surname',\t'NumOfProducts']\n",
    "new_data = data_1\n",
    "#exited = data.pop(\"Exited\")\n",
    "for i in unused_list:\n",
    "    new_data.pop(i)\n",
    "\n",
    "\n",
    "new_data = new_data.sample(frac= 1 , random_state=2)\n",
    "new_data = pd.get_dummies(new_data)\n",
    "\n",
    "\n",
    "def Scaler(new_data , ratio = 9000):\n",
    "    train_data = new_data[:ratio]\n",
    "    test_data = new_data[ratio:]\n",
    "    train_data_val = train_data.pop('Exited')\n",
    "    test_data_val = test_data.pop(\"Exited\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data.to_numpy())\n",
    "    Y_train = train_data_val.to_numpy()\n",
    "    X_test_scaled = scaler.fit_transform(test_data.to_numpy())\n",
    "    Y_test = test_data_val.to_numpy()\n",
    "    return X_train_scaled , Y_train , X_test_scaled , Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_g(X_train_scaled , y_train , X_test_scaled , y_test , PLOT = False):\n",
    "    forest_base = RandomForestRegressor(n_estimators=50, max_depth=5).fit(X_train_scaled, y_train)\n",
    "\n",
    "    #print (forest_base_test_predictions)\n",
    "    return forest_base\n",
    "\n",
    "def LR(X_train_scaled , y_train , X_test_scaled , y_test , PLOT = False , history = ''):\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compute_metrics(y_pred , PLOT ,y_test = None ):\n",
    "    accuracy = accuracy_score(y_test , (y_pred >0.5 ).astype(int) )\n",
    "    precision = precision_score(y_test, (y_pred >0.5 ).astype(int))\n",
    "    recall = recall_score(y_test ,(y_pred >0.5 ).astype(int))\n",
    "    f1 = f1_score(y_test,(y_pred >0.5 ).astype(int))\n",
    "    roc_auc = roc_auc_score(y_test, (y_pred >0.5 ).astype(int))\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print ((\"Here are the results:\\nAccuracy {}\\nPrecission {}\\nRecall {}\\nF1 score {}\\nroc_acc {}\\nThe MAe{}\\nconf_matrix {}\\n{}\").format(accuracy , precision,recall , f1 ,roc_auc ,mae, conf_matrix , sum(conf_matrix[0])))\n",
    "\n",
    "\n",
    "    return mae\n",
    "\n",
    "def Conf_matrix(y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)  # y_test should be your true labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"size\": 16})\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def neural_net(X_train_scaled , y_train , PLOT = False , num_epochs = 15 , input_shape =12 ):\n",
    "    # Build the neural network\n",
    "    \n",
    "   \n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(input_shape,), kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),  # Dropout layer with a 50% dropout rate\n",
    "        keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),  # Dropout layer with a 50% dropout rate\n",
    "        keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "   \n",
    "\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "    #history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2 ,  callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "    if PLOT:\n",
    "        training_loss = history.history['loss']\n",
    "        training_accuracy = history.history['accuracy']\n",
    "        validation_loss = history.history['val_loss']\n",
    "        validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "        # Plot the training and validation loss\n",
    "        plt.plot(training_loss, label='Training Loss')\n",
    "        plt.plot(validation_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the training and validation accuracy\n",
    "        plt.plot(training_accuracy, label='Training Accuracy')\n",
    "        plt.plot(validation_accuracy, label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Evaluation\n",
    "    return model\n",
    "    #loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "\n",
    "def pca_trans(X_train , X_test , n_comp = 10):\n",
    "    from sklearn.decomposition import PCA\n",
    "    shape_1 = X_train.shape\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca = pca.fit(X_train)\n",
    "\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    shape_2 = X_train_pca.shape\n",
    "    print (f\"INPUT DATA SHAPE WAS {shape_1}  final shape = {shape_2}\")\n",
    "    return X_train_pca , X_test_pca\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
